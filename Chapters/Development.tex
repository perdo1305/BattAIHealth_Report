%************************************************
\chapter{Development}
\label{ch:Development}
%************************************************

\section{Introduction}

This chapter details the development phases of the battery health prediction system, encompassing the complete progression from initial MATLAB modeling to advanced deep learning implementation. The project evolved through distinct phases: (1) MATLAB-based modeling and simulation using traditional methods, (2) exploration of hybrid neural network approaches, (3) transition to pure data-driven models, and (4) implementation of state-of-the-art deep learning architectures for time series forecasting.

The development process was guided by the need to create accurate, robust, and scalable battery health prediction models capable of handling real-world applications. Each phase built upon lessons learned from previous approaches, ultimately leading to the implementation of TimesNet, a cutting-edge time series analysis architecture that demonstrates superior performance in battery degradation prediction tasks.

\section{MATLAB Modeling and Simulation}

The initial development phase focused on implementing traditional battery modeling approaches in MATLAB to establish baseline performance and understand the fundamental characteristics of battery degradation patterns.

\textbf{Kalman Filter Implementation}

The Extended Kalman Filter (EFK) was implemented as the primary estimation algorithm for SOC and SOH prediction. The EFK approach was chosen for its proven effectiveness in handling the nonlinear dynamics of battery systems and its ability to provide uncertainty quantification.

\textbf{Coulomb Counting Integration}

Coulomb counting was integrated as a complementary method for SOC estimation, providing a reference baseline for comparison with the Kalman filter results. The implementation addressed several critical considerations to ensure accuracy and reliability. \textbf{Current integration accuracy and drift compensation} were prioritized to minimize cumulative errors that could significantly impact SOC estimates over extended periods. \textbf{Temperature effects on coulombic efficiency} were carefully analyzed, as thermal variations can substantially alter the charge-discharge efficiency and affect the accuracy of capacity calculations. \textbf{Aging effects on capacity estimation} were incorporated to account for the gradual degradation of battery capacity over operational lifetime, ensuring that SOC estimates remain accurate as the battery ages. Finally, \textbf{calibration procedures for initial SOC determination} were established to provide accurate baseline measurements, which are crucial for the cumulative nature of coulomb counting methods.

\textbf{Batemo Model Integration}

The Batemo battery model was incorporated to provide physics-based battery behavior simulation. This integration offered comprehensive capabilities for model development and validation. \textbf{Validation of estimation algorithms under controlled conditions} was enabled through the model's ability to simulate precise battery behaviors, allowing for systematic testing of algorithm performance across various operational scenarios. \textbf{Generation of synthetic data for algorithm testing} provided a valuable resource for training and evaluating neural networks when real-world data was limited or when specific degradation patterns needed to be studied. \textbf{Analysis of model sensitivity to various degradation mechanisms} was facilitated by the physics-based nature of the Batemo model, enabling detailed investigation of how different aging phenomena affect battery performance predictions. Additionally, \textbf{comparison between model-based and data-driven approaches} was made possible, allowing for comprehensive evaluation of different estimation methodologies and their respective strengths and limitations.

The MATLAB implementation served as a foundation for understanding battery dynamics and provided insights that informed subsequent neural network development phases.

\section{Neural Network Development Evolution}

The neural network development process evolved through multiple iterations, each addressing specific limitations identified in previous approaches and incorporating lessons learned from the MATLAB modeling phase.

\textbf{Initial CNN+LSTM Architecture}

The first neural network implementation combined CNN with LSTM networks to leverage both spatial feature extraction and temporal sequence modeling capabilities.

\textbf{Architecture Design}

The CNN+LSTM architecture was structured to leverage the complementary strengths of both convolutional and recurrent neural networks. The design incorporated multiple specialized layers, each serving a distinct purpose in the feature extraction and temporal modeling pipeline. \textbf{CNN layers} were responsible for extracting local patterns and features from battery measurement sequences, identifying spatial relationships and important signal characteristics within the input data. \textbf{LSTM layers} focused on modeling long-term dependencies and temporal relationships, capturing the sequential nature of battery degradation and state evolution over time. Finally, \textbf{Dense layers} provided the final prediction mapping with appropriate activation functions, transforming the processed features into accurate SOC and SOH estimates.

\textbf{Training Challenges and Limitations}

Several significant challenges were encountered during the CNN+LSTM implementation that highlighted the complexity of applying deep learning to battery health monitoring. These obstacles required careful analysis and ultimately influenced the decision to pursue alternative approaches. \textbf{Gradient vanishing} emerged as a critical issue where long sequences caused training instability, preventing the network from effectively learning long-term dependencies essential for accurate battery state prediction. \textbf{Overfitting} presented another major concern, as the high model complexity led to poor generalization, with the network memorizing training patterns rather than learning transferable features applicable to new battery data. \textbf{Computational efficiency} posed practical limitations, with training time becoming prohibitively long for large datasets, making the approach unsuitable for real-world applications requiring timely model updates. Additionally, \textbf{Feature engineering} proved challenging, as manual feature selection proved suboptimal, requiring extensive domain expertise and iterative refinement that limited the model's adaptability to different battery types and operating conditions.

\textbf{Hybrid vs. Data-Driven Approach Evaluation}

A systematic comparison was conducted between hybrid approaches (combining physics-based models with neural networks) and pure data-driven methods.

\textbf{Hybrid Approach Implementation}

The hybrid approach integrated multiple complementary techniques to leverage both physics-based understanding and data-driven learning capabilities. This comprehensive strategy incorporated several key components designed to enhance prediction accuracy and reliability. \textbf{Physics-based model outputs as additional input features} provided domain-specific insights that enriched the neural network's understanding of battery behavior, incorporating fundamental electrochemical principles into the learning process. \textbf{Kalman filter estimates as regularization terms} helped constrain the neural network training by incorporating well-established state estimation techniques, reducing the likelihood of unrealistic predictions and improving model stability. Furthermore, \textbf{domain knowledge constraints in loss function design} ensured that the learned models respected known physical limitations and relationships, preventing the network from learning patterns that violated fundamental battery physics principles.

\textbf{Data-Driven Approach Focus}

The pure data-driven approach emphasized maximum utilization of machine learning capabilities while minimizing reliance on explicit domain knowledge. This methodology focused on allowing the neural network to discover patterns and relationships directly from the data. \textbf{End-to-end learning from raw sensor data} enabled the model to process unfiltered battery measurements, potentially capturing subtle patterns that might be lost during manual preprocessing or feature engineering steps. \textbf{Automatic feature extraction and representation learning} allowed the network to identify the most relevant characteristics for battery health prediction without requiring extensive domain expertise or manual feature design. Additionally, \textbf{minimal domain-specific preprocessing requirements} reduced implementation complexity and improved the method's adaptability to different battery types and measurement configurations, making it more suitable for diverse real-world applications.

\textbf{Comparative Analysis Results}

The evaluation revealed that data-driven approaches demonstrated \textbf{superior generalization} across different battery chemistries, \textbf{reduced dependency} on accurate physics model parameters, \textbf{better scalability} to large datasets, and \textbf{more robust performance} under varying operating conditions.

This analysis motivated the transition to advanced pure data-driven architectures.

\section{Dataset Collection and Preprocessing}

The dataset development process was crucial for training robust and generalizable models, requiring careful consideration of data quality, diversity, and preprocessing strategies.

\textbf{Primary Dataset Selection}

The primary dataset utilized in this work is the CALCE (Center for Advanced Life Cycle Engineering) battery dataset, obtained from the University of Maryland's public repository. This dataset was selected for its:

\begin{itemize}
    \item Comprehensive cycle life data spanning multiple battery chemistries
    \item High-quality measurements with consistent sampling rates
    \item Well-documented experimental conditions and procedures
    \item Established use in battery research community for benchmarking
\end{itemize}

\textbf{Data Cleaning and Preprocessing}

Several preprocessing steps were implemented to ensure data quality and model training stability:

\textbf{Cycle Removal and Filtering}

\begin{itemize}
    \item \textbf{Initial cycle removal}: The first charge-discharge cycle was removed from each battery file to avoid initialization artifacts and inconsistent starting conditions
    \item \textbf{Incomplete cycle filtering}: Cycles with insufficient data points or incomplete charge/discharge sequences were excluded from the training set
    \item \textbf{Outlier detection}: Statistical methods were applied to identify and remove measurement outliers that could negatively impact model training
\end{itemize}
Battery-level partitioning was employed to prevent data leakage, ensuring that cycles from the same battery appeared in only one partition.

\textbf{Input Data Formatting}

The input data structure was designed to optimize model performance:

\begin{itemize}
    \item \textbf{Sequence length}: Fixed-length sequences of 100 time steps were extracted from continuous battery operation data
    \item \textbf{Feature vector}: Each time step included voltage, current, temperature, and derived features such as power and energy
    \item \textbf{Target variables}: State of Health (SOH) values were calculated based on capacity fade measurements
    \item \textbf{Sliding window}: Overlapping sequences were generated to maximize training data utilization
\end{itemize}

\textbf{Limitations of Previous Approaches}

The evaluation of CNN+LSTM and hybrid approaches revealed several critical limitations:

\begin{itemize}
    \item \textbf{Temporal modeling constraints}: Traditional LSTM architectures struggled with very long sequences typical in battery degradation analysis
    \item \textbf{Multi-scale pattern recognition}: Difficulty in capturing both short-term fluctuations and long-term degradation trends simultaneously
    \item \textbf{Computational efficiency}: High computational requirements limited the scalability to large datasets
    \item \textbf{Generalization issues}: Poor performance when applied to battery chemistries or operating conditions not seen during training
\end{itemize}

\textbf{Requirements for Advanced Architecture}

The identified requirements for an improved architecture included:

\begin{itemize}
    \item \textbf{Multi-periodicity detection}: Ability to automatically identify and exploit multiple periodic patterns in battery data
    \item \textbf{Long-range dependency modeling}: Effective capture of dependencies across extended time horizons
    \item \textbf{Parameter efficiency}: Reduced model complexity while maintaining or improving performance
    \item \textbf{Versatility}: Capability to handle various time series analysis tasks beyond just forecasting
\end{itemize}

These requirements led to the selection and implementation of TimesNet, a cutting-edge architecture specifically designed for general time series analysis.

\section{Utilized Model (TimesNet)}

TimesNet is a state-of-the-art neural network architecture specifically designed for general time series analysis tasks~\cite{wu_timesnet_2023}. This model addresses the fundamental challenge of temporal variation modeling by transforming the complex problem from 1D time series analysis into 2D space analysis. The key innovation of TimesNet lies in its ability to discover multi-periodicity patterns in time series data and decompose intricate temporal variations into intraperiod and interperiod variations.

The architecture works by converting 1D time series into a set of 2D tensors based on multiple identified periods. This transformation embeds intraperiod variations into the columns and interperiod variations into the rows of the 2D tensors, making temporal patterns more accessible for analysis through 2D convolution operations. The core component, TimesBlock, can adaptively discover multi-periodicity and extract complex temporal variations using parameter-efficient inception blocks.

TimesNet demonstrates superior performance across five mainstream time series analysis tasks: short-term and long-term forecasting, imputation, classification, and anomaly detection. This versatility makes it particularly suitable for battery health prediction tasks, where complex temporal dependencies and multi-scale patterns are crucial for accurate state-of-health estimation. The model's ability to handle various sequence lengths and its robust architecture for capturing temporal dynamics align well with the requirements of battery degradation modeling, where both short-term fluctuations and long-term trends must be considered simultaneously.

\textbf{Architecture Adaptation for Battery Health Prediction}

The TimesNet architecture was adapted for battery health prediction with several key modifications:

\begin{itemize}
    \item \textbf{Input preprocessing}: Battery measurement sequences (voltage, current, temperature) were formatted to exploit the multi-periodicity detection capabilities
    \item \textbf{Output configuration}: Modified for regression tasks to predict continuous SOH values rather than classification outputs
    \item \textbf{Loss function}: Implemented Mean Squared Error (MSE) with additional regularization terms to prevent overfitting
    \item \textbf{Feature engineering}: Minimal manual feature engineering to leverage the model's automatic pattern discovery capabilities
\end{itemize}

\section{Model Optimization}

For model optimization, the Optuna tool was utilized, which enables hyperparameter optimization for machine learning models, integrated with Weights \& Biases (WandB), which allows for result visualization and model comparison.

\textbf{Dataset Preparation for Optimization}

For this test, the dataset was reduced to only 1/10 of the data, equally distributed from the original dataset, with the objective of reducing the time required for finding the best hyperparameters, since this process took approximately one week even with this reduction.

\textbf{Optimization Process}

For the hyperparameter search, 50 trials were performed, with 50 epochs each, using an early stopping patience of 5 epochs to avoid overfitting and accelerate the optimization process.

\textbf{Optimized Parameters}

The parameters that were optimized through Optuna include:

\begin{itemize}
    \item \textbf{e\_layers}: Number of encoder layers (1--3) --- controls the depth of the encoder stack
    \item \textbf{d\_layers}: Number of decoder layers (1--3) --- controls the depth of the decoder stack  
    \item \textbf{factor}: Expansion factor for the FFN (1--5) --- controls the complexity of frequency components in TimesNet
    \item \textbf{freq}: Frequency for time features encoding (``s'', ``t'', ``h'') --- seconds, minutes, hours
    \item \textbf{d\_model}: Model dimension (fixed at 16)
    \item \textbf{top\_k}: Top-k dominant frequencies in TimesNet (1--5) --- controls how many frequency components to consider
\end{itemize}

\textbf{Parameter Importance Analysis}

Through this optimization, it was possible to detect the importance of the hyperparameters. We observed that the importance factor of the \textbf{e\_layers} parameter (number of encoder layers) is the parameter that most influences the result when changed, demonstrating that the depth of the encoder architecture is critical for model performance.

\textbf{Best Trial Results}

The most successful trial was trial 15, which presented the following results:

\begin{itemize}
    \item \textbf{MSE Value}: 0.0015545075293630362
    \item \textbf{Optimal Parameters}:
    \begin{itemize}
        \item e\_layers: 2
        \item factor: 4  
        \item d\_model: 16
        \item top\_k: 9
        \item n\_heads: 16
    \end{itemize}
    \item \textbf{Duration}: 7770232 ms (approximately 2 hours and 10 minutes)
\end{itemize}

The results show that using 2 encoder layers works better than deeper networks, likely avoiding overfitting on the battery dataset. The high expansion factor of 4 allows the model to capture more complex patterns, while setting top\_k to 9 means the model considers more frequency components than the default range, which helps capture the various periodic behaviors in battery degradation cycles.
